{
  "model_configs": {
    "generator": {
      "model_name": "distilgpt2",
      "max_length": 100,
      "num_return_sequences": 5,
      "temperature": 0.8,
      "top_k": 50,
      "top_p": 0.9
    },
    "embeddings": {
      "model_name": "all-MiniLM-L6-v2"
    },
    "validation_thresholds": {
      "min_uniqueness_ratio": 0.7,
      "max_exact_duplicates": 0.05,
      "min_avg_perplexity": 5.0,
      "max_avg_perplexity": 100.0,
      "min_embedding_similarity": 0.3
    }
  },
  "lora_config": {
    "r": 16,
    "lora_alpha": 32,
    "target_modules": [
      "c_attn",
      "c_proj"
    ],
    "lora_dropout": 0.1,
    "bias": "none",
    "task_type": "CAUSAL_LM"
  },
  "training_config": {
    "num_train_epochs": 3,
    "per_device_train_batch_size": 4,
    "per_device_eval_batch_size": 4,
    "warmup_steps": 100,
    "weight_decay": 0.01,
    "logging_steps": 10,
    "save_steps": 500,
    "eval_steps": 500,
    "save_total_limit": 2,
    "load_best_model_at_end": true,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false
  }
}